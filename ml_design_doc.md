# Название:
Автоматическая фильтрация нецелевых задач в работе аналитика бизнесс-процессов методами машинного обучения с учителем

## 1. Обзор
Цель: Сократить долю нецелевых заявок до 10%
Задачи:
- Собрать и проанализировать данные по релевантным заявкам
- Выбрать подходящие для задачи алгоритмы
- Применить алгоритмы на данных и сравнить результаты
- Предоставить итоги анализа профильной команде

## 2. Мотивация
Эта проблема важна, так как нецелевые заявки тратят рабочее время сотрудников впустую - а это сказывается на эффективности работы, что влияет на деньги компании

## 3. Метрики
- ROC AUC - использую для поиска наилучшей модели
- FPR - удерживаю на незначительном уровне, чтобы поймать ощутимый эффект

## 4. Требования
- Жестких требований со стороны бизнеса нет, так как в эту сторону раньше не было исследований.
В процессе разработки решения пришел к мысли установить FPR на уровне 10%, так как мы не сможем свести FP к 0, а высокий показатель будет нам дорого стоить.
- Так как сами заявки составляются живыми людьми, то есть понятный риск, что практически всегда часть заявок может быть неправильно классифицирована.
- Сложно добиться идеальной классификации заявок, так как тексты в разные моменты могут меняться и модели могут не улавливать контекст.

## 5. Методология
### 5.1 Постановка проблемы
Перед нами задача бинарной текстовой классификации: нужно автоматически определять, относится ли заявка в Service Desk к компетенции аналитиков (целевые заявки) или нет (нецелевые заявки). Это позволит перенаправлять нецелевые заявки сразу в нужный отдел, минимизируя потери времени аналитиков на их обработку.
### 5.2 Данные
Для обучения модели используется датасет заявок в SD, поступивших в профильный отдел аналитики за период с января 2024 по март 2025 года.
В датасете два столбца:
- текстовое описание заявки
- способ закрытия заявки (1 — заявка отклонена, 0 — заявка выполнена аналитиком)
----
- Класс 1 (нецелевые) — 9038 наблюдений
- Класс 0 (целевые) — 5743 наблюдения

Во время Инференса на вход модели также будет подаваться текст заявки.
### 5.3 Техника
Для базового сравнения использовался случайный классификатор (эксперимент с подбрасыванием монетки).

В качестве первой модели применялся Наивный байесовский классификатор, учитывая его эффективность для задач тематической классификации текстов и спам-фильтрации.

Для повышения точности была использована модель CatBoost, обученная на предварительно размеченных и очищенных данных.

Также пробовал E5_large_p_tuning (prefix tuning).

В E5 и Catboost векторизация текста происходила внутри модели (отдельно не настраивалась).
### 5.4 Эксперименты и валидация
Оценка качества моделей проводилась оффлайн с использованием метрики ROC-AUC для комплексной оценки качества классификации, а также Accuracy для дополнительного контроля.

В качестве допустимого уровня ложноположительных ошибок (FPR) установлен порог не более 5-10%, исходя из бизнес-требований по минимизации нагрузки на аналитиков.

CatBoost показал ROC-AUC 0.75, Accuracy 0.813 на обучающем наборе, что существенно лучше базовой и наивной модели.

Для дальнейшей валидации планируется пилотное тестирование на ограниченной выборке с последующим анализом бизнес-метрик (например, снижение доли нецелевых заявок у аналитиков, экономия времени).
### 5.5 Human-in-the-loop
На этапе внедрения и пилотного тестирования сохраняется возможность ручной проверки и корректировки результатов модельных предсказаний аналитиками.

Планируется формирование списка исключений (например, специфические типы заявок, которые всегда требуют ручной обработки), а также сбор обратной связи для дообучения и уточнения модели.

В перспективе возможна интеграция механизма ручного подтверждения для спорных или низкоуверенных предсказаний модели.
